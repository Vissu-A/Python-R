{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            A         B         C         D\n",
      "2019-05-15 00:00:20 -0.632673 -0.076356  0.740482 -1.293758\n",
      "2019-05-15 00:00:20 -0.629153  0.142680  1.197175  1.426024\n",
      "2019-05-15 00:00:40  0.014624 -0.939138  0.379475  1.238219\n",
      "2019-05-15 00:00:38 -0.057139  1.169432  0.991922 -1.161098\n",
      "2019-05-15 00:00:58  1.140074 -1.870670  0.148647  0.160543\n",
      "2019-05-15 00:00:55 -0.623893 -0.821005 -0.156214  0.364467\n",
      "2019-05-15 00:01:15  0.407902 -0.082176 -0.336603 -0.495715\n",
      "2019-05-15 00:01:15  1.089271  0.682740 -0.569875  1.473240\n",
      "2019-05-15 00:01:35 -0.755965 -0.734179 -0.645755  0.761400\n",
      "2019-05-15 00:01:33  0.474224 -0.365034  0.564351 -1.815273\n",
      "2019-05-15 00:01:53 -1.159814  0.079918  0.907609  1.376399\n",
      "2019-05-15 00:01:53 -0.646965  1.257073  0.974703  0.198590\n",
      "2019-05-15 00:02:13  0.482354  1.012595  2.127562  0.543398\n",
      "2019-05-15 00:02:13  0.919450  0.178667 -0.681692  0.436830\n",
      "2019-05-15 00:02:33 -0.786295 -0.028471 -0.205520  0.758686\n",
      "2019-05-15 00:02:31 -0.169933  0.494694  0.287569  1.518251\n",
      "2019-05-15 00:02:51 -2.253336  1.367935  1.271937  0.328447\n",
      "Index(['A', 'B', 'C', 'D'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dates = []\n",
    "date  = datetime.datetime(2019, 5, 15)\n",
    "\n",
    "for i in range(17):\n",
    "\n",
    "    if(i%2 == 0):\n",
    "        ndate = date + datetime.timedelta(hours=0, minutes = 0, seconds = 20)\n",
    "    elif(i%3 == 0):\n",
    "        ndate = date - datetime.timedelta(hours=0, minutes = 0, seconds = 2)\n",
    "    elif(i%4 == 0):\n",
    "        ndate = date - datetime.timedelta(hours=0, minutes = 0, seconds = 10)\n",
    "    elif(i%5 == 0):\n",
    "        ndate = date - datetime.timedelta(hours=0, minutes = 0, seconds = 3)\n",
    "    else:\n",
    "        ndate = date + datetime.timedelta(hours=0 ,minutes=0, seconds = 0)\n",
    "        \n",
    "    dates.append(ndate)\n",
    "    date = ndate\n",
    "\n",
    "dates\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(17, 4), index=dates, columns=['A', 'B', 'C', 'D'])\n",
    "print(df)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting timestamp column values into datetime type which may come in string format and then sorting the dates and setting dates column as index\n",
    "def clean_ts_col(df, ts_col):\n",
    "    # Sort timestamp and set timestamp as index\n",
    "    df[ts_col] = pd.to_datetime(df[ts_col])\n",
    "    df = df.sort_values(ts_col).set_index(ts_col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_ts(df, host_name):\n",
    "    # Remove empty columns.\n",
    "    df = df.dropna(axis=1, thresh=1)\n",
    "    logger.logger.info('Info: Empty columns dropped.')\n",
    "    \n",
    "    # Convert numeric columns to float where possible, since data came in as strings (JSON).\n",
    "    cols_to_convert = [c for c in df.columns if c!= host_name]\n",
    "    for c in cols_to_convert:\n",
    "        try:\n",
    "            df[c] = df[c].astype(float)\n",
    "            # logger.info('Info: Data type converted to numeric.')\n",
    "        except:\n",
    "            logger.logger.warning('Warning: Data type cannot be converted to numeric in column {}.'.format(c))\n",
    "            pass\n",
    "        \n",
    "    # Break each column, remove missing values, return as a list\n",
    "    try:\n",
    "        # Drop the missing value for each column\n",
    "        vm_metrics_list = [df[c].dropna() for c in df.columns]\n",
    "        # logger.info('Info: Sparse columns are converted to a list of time series for this node.')\n",
    "    except:\n",
    "        logger.exception('Error: Cannot convert sparse columns into time series.')\n",
    "    return vm_metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sampling_freq(ts):\n",
    "    # assert ts.index is datetime object\n",
    "    try:\n",
    "        # For non-NA values, check time difference in seconds in each timestamp record for each metric\n",
    "        sf = pd.Series(ts.index).diff().dropna().dt.seconds\n",
    "        # logger.info('Info: Found natural sampling frequency')\n",
    "    except:\n",
    "        logger.exception('Error: Cannot find natural sampling frequency.')\n",
    "        \n",
    "    # Only keep record with timestamp difference greater than 0 seconds\n",
    "    sf = sf[sf > 0]\n",
    "    # if the series has different timestamp, take the mininum value from the mode as the sampling frequency\n",
    "    if len(sf) > 0:\n",
    "        sampling_frequency = int(sf.mode().values.min())\n",
    "    # Otherwise, sampling frequency is 0\n",
    "    else:\n",
    "        sampling_frequency = 0\n",
    "    return ts, sampling_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_ts(ts, orig_frequency, new_frequency):\n",
    "    # Compare the calculated sampling frequency with the defined re-sample frequency\n",
    "    # resample\n",
    "    print('Original frequency:', orig_frequency)\n",
    "    print('New frequency:', new_frequency)\n",
    "    if orig_frequency <= new_frequency:\n",
    "        # print(ts.name, new_frequency)\n",
    "        if ts.dtypes != 'object':\n",
    "            ts_new = ts.resample(str(new_frequency)+'s').first().interpolate()\n",
    "        else:\n",
    "            ts_new = ts.resample(str(new_frequency)+'s').first()\n",
    "    else:\n",
    "        # print(ts.name, orig_frequency)\n",
    "        if ts.dtypes != 'object':\n",
    "            ts_new = ts.resample(str(orig_frequency)+'s').first().interpolate()\n",
    "        else:\n",
    "            ts_new = ts.resample(str(orig_frequency)+'s').first()\n",
    "    # print(ts_new.name, ts_new.dtypes)\n",
    "    return ts_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_by_endpoint(vm, ts_col, host_name, new_frequency):\n",
    "    # each 'vm' is a df of relevant metrics for an individual endpoint\n",
    "    # Sort timestamp and set timestamp as index\n",
    "    # vm = clean_ts_col(vm, ts_col)\n",
    "    # Convert numeric columns to float if necessary, drop null value for each column and return a list of list for each metric\n",
    "    vm_metrics_list = get_metrics_ts(vm, host_name)\n",
    "    # For each metric, compute the sampling frequency. It returns vm_metrics_list and its corresponding sampling frequency\n",
    "    samp_freqs = [find_sampling_freq(met) for met in vm_metrics_list]\n",
    "    # Compare the calculated sampling frequency with the pre-defined re-sample frequency, resample and get new vm_metrics_list\n",
    "    vm_metrics_list = [aggregate_ts(ts, of, new_frequency) for ts, of in samp_freqs]\n",
    "    return vm_metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/May/2019 18:21:43] INFO - Info: Empty columns dropped.\n",
      "Original frequency: 20\n",
      "New frequency: 30\n",
      "Original frequency: 20\n",
      "New frequency: 30\n",
      "Original frequency: 20\n",
      "New frequency: 30\n",
      "Original frequency: 20\n",
      "New frequency: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2019-05-15 00:00:00   -0.632673\n",
       " 2019-05-15 00:00:30   -0.057139\n",
       " 2019-05-15 00:01:00    0.407902\n",
       " 2019-05-15 00:01:30    0.474224\n",
       " 2019-05-15 00:02:00    0.482354\n",
       " 2019-05-15 00:02:30   -0.169933\n",
       " Freq: 30S, Name: A, dtype: float64, 2019-05-15 00:00:00   -0.076356\n",
       " 2019-05-15 00:00:30    1.169432\n",
       " 2019-05-15 00:01:00   -0.082176\n",
       " 2019-05-15 00:01:30   -0.365034\n",
       " 2019-05-15 00:02:00    1.012595\n",
       " 2019-05-15 00:02:30    0.494694\n",
       " Freq: 30S, Name: B, dtype: float64, 2019-05-15 00:00:00    0.740482\n",
       " 2019-05-15 00:00:30    0.991922\n",
       " 2019-05-15 00:01:00   -0.336603\n",
       " 2019-05-15 00:01:30    0.564351\n",
       " 2019-05-15 00:02:00    2.127562\n",
       " 2019-05-15 00:02:30    0.287569\n",
       " Freq: 30S, Name: C, dtype: float64, 2019-05-15 00:00:00   -1.293758\n",
       " 2019-05-15 00:00:30   -1.161098\n",
       " 2019-05-15 00:01:00   -0.495715\n",
       " 2019-05-15 00:01:30   -1.815273\n",
       " 2019-05-15 00:02:00    0.543398\n",
       " 2019-05-15 00:02:30    1.518251\n",
       " Freq: 30S, Name: D, dtype: float64]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_by_endpoint(df,'no ts column','no host name', 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
